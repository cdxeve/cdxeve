# Daixuan Cheng

- I am a **Ph.D. student** at the Gaoling School of AI, Renmin University of China, fortunately advised by [Wayne Xin Zhao](https://scholar.google.com/citations?user=JNhNacoAAAAJ&hl=en).  

- Ever since 2021, I have been a **research student** fortunately advised by [Shaohan Huang](https://buaahsh.github.io) and [Furu Wei](https://thegenerality.com) from the GenAI Group of Microsoft Research, with whom I have accomplished many of my representative works!!!

- I was previously a **research assistant** in the CoAI Group, Tsinghua University, fortunately advised by [Yuxian Gu](https://t1101675.github.io) and [Minlie Huang](https://scholar.google.com/citations?user=P1jPSzMAAAAJ&hl=zh-CN).  

- Before that, I received my **Bachelor's degree in Communication Engineering** and **Master's degree in Computer Science** from Beijing University of Posts and Telecommunications, fortunately advised by [Haifeng Sun](https://hfsun.github.io).  

- I also worked as a **research engineer** at BIGAI, fortunately collaborating with [Xuekai Zhu](https://xuekai-zhu.github.io/Xuekai-Zhu/).  


## Research Interests  

I am dedicated to enhancing **Large Language Models (LLMs)** across their entire lifecycle, including:  
- **Pre-training**: [Instruction Pre-Training](https://arxiv.org/abs/2406.14491), [AdaptLLM](https://arxiv.org/abs/2309.09530), [VL-Match](https://openaccess.thecvf.com/content/ICCV2023/papers/Bi_VL-Match_Enhancing_Vision-Language_Pretraining_with_Token-Level_and_Instance-Level_Matching_ICCV_2023_paper.pdf).  
- **Domain Adaptation**: [AdaptLLM](https://arxiv.org/abs/2309.09530), [AdaMLLM](https://arxiv.org/abs/2411.19930), [SODA](https://aclanthology.org/2022.findings-emnlp.163/).  
- **Reasoning and Reinforcement Learning**: [Reasoning with Exploration](https://arxiv.org/abs/2506.14758), [STILL](https://github.com/RUCAIBox/Slow_Thinking_with_LLMs).  
- **Synthetic Data**: [Instruction Pre-Training](https://arxiv.org/abs/2406.14491), [AdaptLLM](https://arxiv.org/abs/2309.09530), [ToEdit](https://arxiv.org/abs/2412.14689).  


## Selected Publications  
*(Full list at [Google Scholar](https://scholar.google.com/citations?hl=en&user=flRAZJQAAAAJ&view_op=list_works))*

* **Daixuan Cheng**, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, Furu Wei. Reasoning with exploration: An entropy perspective. *Arxiv Preprint, 2025*. [pdf](https://arxiv.org/abs/2506.14758) (**Exploration of RLVR, LLM Reasoning**)

* **Daixuan Cheng**, Shaohan Huang, Ziyu Zhu, Xintong Zhang, Wayne Xin Zhao, Zhongzhi Luan, Bo Dai, Zhenliang Zhang. On Domain-Adaptive Post-Training for Multimodal Large Language Models. *EMNLP 2025 (Findings, Long Paper)*. [pdf](https://arxiv.org/abs/2411.19930) [code](https://github.com/bigai-ai/QA-Synthesizer) [huggingface](https://huggingface.co/AdaptLLM/Adapt-MLLM-to-Domains) (**Domain Adaptation of Multimodal LLM**)

* **Daixuan Cheng**, Yuxian Gu, Shaohan Huang, Junyu Bi, Minlie Huang, Furu Wei. Instruction Pre-Training: Language Models are Supervised Multitask Learners. *EMNLP 2024 (Main, Long Paper)*. [pdf](https://arxiv.org/abs/2406.14491) [code](https://github.com/microsoft/LMOps/tree/main/instruction_pretrain) [huggingface](https://huggingface.co/instruction-pretrain) (**LLM Pre-Training**)

* **Daixuan Cheng**, Shaohan Huang, Furu Wei. Adapting Large Language Models via Reading Comprehension. *ICLR 2024*. [pdf](https://arxiv.org/abs/2309.09530) [code](https://github.com/microsoft/LMOps/tree/main/adaptllm) [huggingface](https://huggingface.co/AdaptLLM) (**Domain Adaptation of LLM**)

* **Daixuan Cheng**, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu Wei, Denvy Deng, Qi Zhang. Uprise: Universal prompt retrieval for improving zero-shot evaluation. *EMNLP 2023 (Main, Long Paper)*. [pdf](https://arxiv.org/abs/2303.08518) [code](https://github.com/microsoft/LMOps/tree/main/uprise) (**RAG of LLM**)

* **Daixuan Cheng**, Shaohan Huang, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Furu Wei, Denvy Deng, Qi Zhang. Snapshot-guided domain adaptation for ELECTRA. *EMNLP 2022 (Findings, Short Paper)*. [pdf](https://aclanthology.org/2022.findings-emnlp.163/) (**Domain Adaptation of LM**)

* Xuekai Zhu, **Daixuan Cheng**, Hengli Li, Kaiyan Zhang, Ermo Hua, Xingtai Lv, Ning Ding, Zhouhan Lin, Zilong Zheng, Bowen Zhou. How to Synthesize Text Data without Model Collapse? *ICML 2025*. [pdf](https://arxiv.org/abs/2412.14689) [code](https://github.com/Xuekai-Zhu/toedit) (**Synthetic Data for LLM**)

* Junyu Bi, **Daixuan Cheng**, Ping Yao, Bochen Pang, Yuefeng Zhan, Chuanguang Yang, Yujing Wang, Hao Sun, Weiwei Deng, Qi Zhang. VL-Match: Enhancing Vision-Language Pretraining with Token-Level and Instance-Level Matching. *ICCV 2023*. [pdf](https://openaccess.thecvf.com/content/ICCV2023/papers/Bi_VL-Match_Enhancing_Vision-Language_Pretraining_with_Token-Level_and_Instance-Level_Matching_ICCV_2023_paper.pdf) (**Pre-Training of Vision-Language Models**)

* Huazheng Wang, **Daixuan Cheng**, Haifeng Sun, Jingyu Wang, Qi Qi, Jianxin Liao, Jing Wang, Cong Liu. How Does Diffusion Influence Pretrained Language Models on Out-of-Distribution Data? *ECAI 2023*. [pdf](https://arxiv.org/abs/2307.13949) (**Diffusion LM**)

* Haifeng Sun, **Daixuan Cheng**, Jingyu Wang, Qi Qi, Jianxin Liao. Pattern and content controlled response generation. Information Processing & Management 2021 [pdf](https://www.sciencedirect.com/science/article/pii/S0306457321001023) (**Text Generation of LMs**)
 
## Contact  

- ✉️ Email: [daixuancheng6@gmail.com](mailto:daixuancheng6@gmail.com)  





